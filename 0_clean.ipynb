{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign In, Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Cleaning Data\n",
    "# \n",
    "# download the survey's .csv file from limesurvey\n",
    "# remember to always use the .csv file with all the columns (select all columns)\n",
    "# completed answers ('apenas respostas completas')\n",
    "# answers' code ('codigos das respostas')\n",
    "# question's code and complete question ('codigo e texto da pergunta')\n",
    "#rename this file to data_raw_#.csv , being # the number of answers\n",
    "\n",
    "#also export the complete answers of the nationalities and rename it to countries_answers_NUMBEROFANSWERS \n",
    "# ('respostas completas'; 'texto completo da pergunta' da pergunta/coluna NATIONALITY)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data_raw_188.csv')\n",
    "df2 = pd.read_csv('countries_answers_188.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188\n"
     ]
    }
   ],
   "source": [
    "#a ckeck to see if it's actually the correct number of answers\n",
    "num_observations = len(df)\n",
    "\n",
    "print(num_observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  _Well Defined_: No Adjustments Needed After New Raw Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming *Questions* into Clean Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are three main groups of questions: 1. preferences; 2. socio-demographics ; and 3. travel behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the unnecessary columns\n",
    "df = df.drop(['lastpage. Last page', 'seed. Seed', 'startlanguage. Start language','submitdate. Date submitted'], axis=1)\n",
    "\n",
    "#renaming some questions to clean variable names (e.g., socio-demographics questions)\n",
    "df = df.rename(columns={'id. Response ID' : 'id'})\n",
    "df = df.rename(columns={'randGroup. {if(randGroup &gt; 0, randGroup , rand(1,4))}  \\xa0 ' : 'group'})\n",
    "df = df.rename(columns={'CAR. Do you own and have access to a private car?' : 'own_car'})\n",
    "df = df.rename(columns={'PARKING[SQ001]. Do you have access to free parking or do you have a parking permit for your residential area and your workplace/university? [At Home Area]' : 'parking_home'})\n",
    "df = df.rename(columns={'PARKING[SQ002]. Do you have access to free parking or do you have a parking permit for your residential area and your workplace/university? [At Workplace/University]' : 'parking_work'})\n",
    "df = df.rename(columns={'COMMUTETIME. How long does your daily commute approximately take? ' : 'commute_time'})\n",
    "df = df.rename(columns={'AGE. What is your age?\\xa0' : 'age'})\n",
    "df = df.rename(columns={'GENDER. What is your gender identity?' : 'gender'})\n",
    "df = df.rename(columns={'OCCUPATION. What is your current occupation?' : 'occupation'})\n",
    "df = df.rename(columns={'RESIDENCE[SQ001]. Where do you live? [City/Town]' : 'residence_city'})\n",
    "df = df.rename(columns={'RESIDENCE[SQ002]. Where do you live? [Country]' : 'residence_country'})\n",
    "df = df.rename(columns={'NATIONALITY. What is your country of origin?' : 'nationality'})\n",
    "df = df.rename(columns={'CHILDREN. Do you have children?' : 'children'})\n",
    "df = df.rename(columns={'SPECIALIST. Do you have any background, either past or present, through study or work, in the fields of urban mobility, transportation, or urbanism?':'specialist'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming the columns names/variables of the preferences group (preference;context;likert scales)\n",
    "\n",
    "def generate_new_column_names(existing_columns):\n",
    "    new_column_names = {}\n",
    "    num_columns = len(existing_columns)\n",
    "    batch_size = 10\n",
    "    max_batches = 8  # Number of prefixes per hundred series (e.g., G101 to G108)\n",
    "    suffixes = [\n",
    "        '_preference', '_context', '_usefulness', '_safety', '_security', \n",
    "        '_stress', '_comfort', '_order', '_liveability', '_more'\n",
    "    ]\n",
    "    \n",
    "    # Ensure 'group' column is not renamed\n",
    "    new_column_names['group'] = 'group'  # The first column remains \"group\"\n",
    "    \n",
    "    # Start renaming from the second column\n",
    "    start_index = 2  # The first index after 'group'\n",
    "    \n",
    "    for hundred_prefix in range(1, 5):  # G101 to G408 (4 hundred series)\n",
    "        for batch_index in range(max_batches):\n",
    "            prefix = f'G{hundred_prefix * 100 + batch_index + 1}'\n",
    "            end_index = start_index + batch_size\n",
    "            \n",
    "            if start_index >= num_columns:\n",
    "                break\n",
    "\n",
    "            for i in range(start_index, min(end_index, num_columns)):\n",
    "                old_name = existing_columns[i]\n",
    "                suffix_index = i - start_index\n",
    "                new_name = f'{prefix}{suffixes[suffix_index]}'\n",
    "                new_column_names[old_name] = new_name\n",
    "\n",
    "            start_index = end_index\n",
    "\n",
    "    return new_column_names\n",
    "\n",
    "\n",
    "# Generate new column names based on the existing DataFrame columns\n",
    "existing_columns = df.columns\n",
    "new_column_names = generate_new_column_names(existing_columns)\n",
    "\n",
    "# Rename the columns\n",
    "df.rename(columns=new_column_names, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming the columns names/variables of the socio-demographics group (children, pets, disabilities), travel behavior group (Commute), and specialists background (roles)\n",
    "\n",
    "# Define a mapping function based on column patterns\n",
    "def generate_new_column_name(old_name):\n",
    "    if 'CHILDRENNO' in old_name:\n",
    "        age_group = old_name.split('[')[-1].split(']')[0]\n",
    "        return f'children_{age_group}'\n",
    "    elif 'PETS' in old_name:\n",
    "        pet_type = old_name.split('[')[-1].split(']')[0]\n",
    "        return f'pet_{pet_type}'\n",
    "    elif 'DISABILITIES' in old_name:\n",
    "        disability_type = old_name.split('[')[-1].split(']')[0]\n",
    "        return f'disability_{disability_type}'\n",
    "    elif 'SPECIALISTTYPE' in old_name:\n",
    "        role_type = old_name.split('[')[-1].split(']')[0]\n",
    "        return f'role_{role_type}'\n",
    "    elif 'COMMUTE' in old_name:\n",
    "        mode_of_transport = old_name.split('[')[-1].split(']')[0]\n",
    "        return f'commute_{mode_of_transport}'\n",
    "    else:\n",
    "        return old_name  # Return the same name if no match\n",
    "\n",
    "# Create a mapping dictionary\n",
    "new_column_names = {col: generate_new_column_name(col) for col in df.columns}\n",
    "\n",
    "# Rename the columns\n",
    "df.rename(columns=new_column_names, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another adjustment to have the clean variables' names \n",
    "df = df.rename(columns={'commute_I always work from home':'commute_home'})\n",
    "df = df.rename(columns={'children_0 - 5 years old':'children_0-5'})\n",
    "df = df.rename(columns={'children_6 - 10 years old':'children_6-10'})\n",
    "df = df.rename(columns={'children_11 years old and above':'children_11'})\n",
    "df = df.rename(columns={'pet_Yes, I have one (or more) dog(s)':'pet_Y_dog'})\n",
    "df = df.rename(columns={'pet_Yes, I have another type of pet':'pet_Y_other'})\n",
    "df = df.rename(columns={'pet_No, I do not have pets':'pet_N'})\n",
    "df = df.rename(columns={'disability_Yes, I have reduced mobility':'disability_Y_mobility'})\n",
    "df = df.rename(columns={'disability_Yes, I have a visual impairment':'disability_Y_visual'})\n",
    "df = df.rename(columns={'disability_Yes, I have a hearing impairment':'disability_Y_hearing'})\n",
    "df = df.rename(columns={'disability_Yes, I have another type of disability':'disability_Y_other'})\n",
    "df = df.rename(columns={'disability_No, I do not have any disabilities':'disability_N'})\n",
    "df = df.rename(columns={'commute_Taxi/TVDE (e.g. Uber, Bolt)': 'commute_TVDE'})\n",
    "df = df.rename(columns={'role_Academic/Researcher': 'role_academic'})\n",
    "df = df.rename(columns={'role_Professional (e.g., Planner, Engineer)': 'role_professional'})\n",
    "df = df.rename(columns={'role_Government/Public Sector (e.g., city planner, policy maker, transit operator)': 'role_public'})\n",
    "df = df.rename(columns={'role_Industry/Private Sector (e.g., consultant, analyst, developer)': 'role_private'})\n",
    "df = df.rename(columns={'role_ Non-Profit/Advocacy': 'role_nonprofit'})\n",
    "df = df.rename(columns={'role_Technology/Innovation (e.g., developer, data analyst)': 'role_technology'})\n",
    "df = df.rename(columns={'role_Other': 'role_other'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming *Answers* into Clean Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming Values of Preference for All Groups of Interventions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables were renamed the following way:\n",
    " _original value from Limesurvey_ (AO01 or AO02)  →  *X_Y_Z_W* , where:\n",
    "\n",
    "1. X = Intervention_ID (check annexed table, named _FALTAAQUI_)\n",
    "2. Y = Type of Intervention (check annexed table, named _FALTAAQUI_)\n",
    "3. Z = Scale/Dimension of Intervention (l for large; m for medium; t for transformative)\n",
    "4. W = Before Intervention (b) or After Intervention (a)\n",
    "\n",
    "note: the values of A001 and A002 correspond to before or after the intervention, depending on how I've positioned the images in LimeSurvey. To avoid influencing participants’ decisions, the images representing \"before\" and \"after\" for each intervention were placed randomly on either the left or right side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group 1\n",
    "df['G101_preference'] = df['G101_preference'].replace('AO01', '2_pedestrian_l_a')\n",
    "df['G101_preference'] = df['G101_preference'].replace('AO02', '2_pedestrian_l_b')\n",
    "\n",
    "df['G102_preference'] = df['G102_preference'].replace('AO01', '7_shared_m_b')\n",
    "df['G102_preference'] = df['G102_preference'].replace('AO02', '7_shared_m_a')\n",
    "\n",
    "df['G103_preference'] = df['G103_preference'].replace('AO01', '55_pedestrian_l_a')\n",
    "df['G103_preference'] = df['G103_preference'].replace('AO02', '55_pedestrian_l_b')\n",
    "\n",
    "df['G104_preference'] = df['G104_preference'].replace('AO01', '18_transit_m_a')\n",
    "df['G104_preference'] = df['G104_preference'].replace('AO02', '18_transit_m_b')\n",
    "\n",
    "df['G105_preference'] = df['G105_preference'].replace('AO01', '49_free_t_b')\n",
    "df['G105_preference'] = df['G105_preference'].replace('AO02', '49_free_t_a')\n",
    "\n",
    "df['G106_preference'] = df['G106_preference'].replace('AO01', '37_car_t_a')\n",
    "df['G106_preference'] = df['G106_preference'].replace('AO02', '37_car_t_b')\n",
    "\n",
    "df['G107_preference'] = df['G107_preference'].replace('AO01', '33_transit_l_a')\n",
    "df['G107_preference'] = df['G107_preference'].replace('AO02', '33_transit_l_b')\n",
    "\n",
    "df['G108_preference'] = df['G108_preference'].replace('AO02', '30_car_m_a')\n",
    "df['G108_preference'] = df['G108_preference'].replace('AO01', '30_car_m_b')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group 2\n",
    "df['G201_preference'] = df['G201_preference'].replace('AO01', '63_free_t_a')\n",
    "df['G201_preference'] = df['G201_preference'].replace('AO02', '63_free_t_b')\n",
    "\n",
    "df['G202_preference'] = df['G202_preference'].replace('AO01', '60_shared_l_b')\n",
    "df['G202_preference'] = df['G202_preference'].replace('AO02', '60_shared_l_a')\n",
    "\n",
    "df['G203_preference'] = df['G203_preference'].replace('AO01', '37_car_t_a')\n",
    "df['G203_preference'] = df['G203_preference'].replace('AO02', '37_car_t_b')\n",
    "\n",
    "df['G204_preference'] = df['G204_preference'].replace('AO01', '34_car_m_b')\n",
    "df['G204_preference'] = df['G204_preference'].replace('AO02', '34_car_m_a')\n",
    "\n",
    "df['G205_preference'] = df['G205_preference'].replace('AO01', '72_transit_m_b')\n",
    "df['G205_preference'] = df['G205_preference'].replace('AO02', '72_transit_m_a')\n",
    "\n",
    "df['G206_preference'] = df['G206_preference'].replace('AO01', '25_transit_l_a')\n",
    "df['G206_preference'] = df['G206_preference'].replace('AO02', '25_transit_l_b')\n",
    "\n",
    "df['G207_preference'] = df['G207_preference'].replace('AO01', '4_pedestrian_l_b')\n",
    "df['G207_preference'] = df['G207_preference'].replace('AO02', '4_pedestrian_l_a')\n",
    "\n",
    "df['G208_preference'] = df['G208_preference'].replace('AO01', '15_pedestrian_m_b')\n",
    "df['G208_preference'] = df['G208_preference'].replace('AO02', '15_pedestrian_m_a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grupo 3\n",
    "df['G301_preference'] = df['G301_preference'].replace('AO01', '64_car_l_b')\n",
    "df['G301_preference'] = df['G301_preference'].replace('AO02', '64_car_l_a')\n",
    "\n",
    "df['G302_preference'] = df['G302_preference'].replace('AO01', '9_pedestrian_m_b')\n",
    "df['G302_preference'] = df['G302_preference'].replace('AO02', '9_pedestrian_m_a')\n",
    "\n",
    "df['G303_preference'] = df['G303_preference'].replace('AO01', '50_pedestrian_l_b')\n",
    "df['G303_preference'] = df['G303_preference'].replace('AO02', '50_pedestrian_l_a')\n",
    "\n",
    "df['G304_preference'] = df['G304_preference'].replace('AO01', '31_car_m_a')\n",
    "df['G304_preference'] = df['G304_preference'].replace('AO02', '31_car_m_b')\n",
    "\n",
    "df['G305_preference'] = df['G305_preference'].replace('AO01', '69_transit_m_a')\n",
    "df['G305_preference'] = df['G305_preference'].replace('AO02', '69_transit_m_b')\n",
    "\n",
    "df['G306_preference'] = df['G306_preference'].replace('AO01', '66_transit_l_a')\n",
    "df['G306_preference'] = df['G306_preference'].replace('AO02', '66_transit_l_b')\n",
    "\n",
    "df['G307_preference'] = df['G307_preference'].replace('AO01', '39_pedestrian_t_a')\n",
    "df['G307_preference'] = df['G307_preference'].replace('AO02', '39_pedestrian_t_b')\n",
    "\n",
    "df['G308_preference'] = df['G308_preference'].replace('AO01', '67_shared_t_b')\n",
    "df['G308_preference'] = df['G308_preference'].replace('AO02', '67_shared_t_a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grupo 4\n",
    "df['G401_preference'] = df['G401_preference'].replace('AO01', '18_transit_m_a')\n",
    "df['G401_preference'] = df['G401_preference'].replace('AO02', '18_transit_m_b')\n",
    "\n",
    "df['G402_preference'] = df['G402_preference'].replace('AO01', '41_shared_t_a')\n",
    "df['G402_preference'] = df['G402_preference'].replace('AO02', '41_shared_t_b')\n",
    "\n",
    "df['G403_preference'] = df['G403_preference'].replace('AO01', '20_pedestrian_l_b')\n",
    "df['G403_preference'] = df['G403_preference'].replace('AO02', '20_pedestrian_l_a')\n",
    "\n",
    "df['G404_preference'] = df['G404_preference'].replace('AO01', '10_pedestrian_l_b')\n",
    "df['G404_preference'] = df['G404_preference'].replace('AO02', '10_pedestrian_l_a')\n",
    "\n",
    "\n",
    "df['G405_preference'] = df['G405_preference'].replace('AO01', '64_car_l_b')\n",
    "df['G405_preference'] = df['G405_preference'].replace('AO02', '64_car_l_a')\n",
    "\n",
    "df['G406_preference'] = df['G406_preference'].replace('AO01', '69_transit_m_a')\n",
    "df['G406_preference'] = df['G406_preference'].replace('AO02', '69_transit_m_b')\n",
    "\n",
    "df['G407_preference'] = df['G407_preference'].replace('AO01', '62_free_t_a')\n",
    "df['G407_preference'] = df['G407_preference'].replace('AO02', '62_free_t_b')\n",
    "\n",
    "df['G408_preference'] = df['G408_preference'].replace('AO01', '68_car_m_a')\n",
    "df['G408_preference'] = df['G408_preference'].replace('AO02', '68_car_m_b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming Values of Influential Factors on Preference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 Point Likert Scale (with the original values of Limesurvey) to -2 to 2 (extreme negative influence, negative influence, no influence, positive influence, extreme positive influence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maryi\\AppData\\Local\\Temp\\ipykernel_22800\\2930454039.py:22: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[columns_to_replace] = df[columns_to_replace].replace(mapping)\n"
     ]
    }
   ],
   "source": [
    "#im going to change the likert scale now from AO05 - 5 \n",
    "# Mapping dictionary\n",
    "mapping = {\n",
    "    'AO01': -2,\n",
    "    'AO02': -1,\n",
    "    'AO03': 0,\n",
    "    'AO04': 1,\n",
    "    'AO05': 2\n",
    "}\n",
    "\n",
    "# Define the prefixes and the suffixes you want to process\n",
    "prefixes = ['G101', 'G102', 'G103', 'G104', 'G105', 'G106', 'G107', 'G108',\n",
    "            'G201', 'G202', 'G203', 'G204', 'G205', 'G206', 'G207', 'G208',\n",
    "            'G301', 'G302', 'G303', 'G304', 'G305', 'G306', 'G307', 'G308',\n",
    "            'G401', 'G402', 'G403', 'G404', 'G405', 'G406', 'G407', 'G408']\n",
    "suffixes = ['usefulness', 'safety', 'security', 'stress', 'comfort', 'order', 'liveability']\n",
    "\n",
    "# Generate the list of columns to replace\n",
    "columns_to_replace = [f\"{prefix}_{suffix}\" for prefix in prefixes for suffix in suffixes if f\"{prefix}_{suffix}\" in df.columns]\n",
    "\n",
    "# Replace values using the mapping\n",
    "df[columns_to_replace] = df[columns_to_replace].replace(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming Socio-Demographics Answers to Clean Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gender'] = df['gender'].replace('AO01', 'F')\n",
    "df['gender'] = df['gender'].replace('AO02', 'M')\n",
    "df['gender'] = df['gender'].replace('AO03', 'N')\n",
    "df['gender'] = df['gender'].replace('AO04', 'O')\n",
    "\n",
    "\n",
    "df['commute_time'] = df['commute_time'].replace('AO01', '<15')\n",
    "df['commute_time'] = df['commute_time'].replace('AO02', '15-30')\n",
    "df['commute_time'] = df['commute_time'].replace('AO03', '30-45')\n",
    "df['commute_time'] = df['commute_time'].replace('AO05', '+45')\n",
    "df['commute_time'] = df['commute_time'].replace('AO06', 'home')\n",
    "#home = 0 in the scale\n",
    "\n",
    "\n",
    "df['parking_home'] = df['parking_home'].replace('AO01', 'free')\n",
    "df['parking_home'] = df['parking_home'].replace('AO02', 'permit')\n",
    "df['parking_home'] = df['parking_home'].replace('AO03', 'not free')\n",
    "\n",
    "df['parking_work'] = df['parking_work'].replace('AO01', 'free')\n",
    "df['parking_work'] = df['parking_work'].replace('AO02', 'permit')\n",
    "df['parking_work'] = df['parking_work'].replace('AO03', 'not free')\n",
    "\n",
    "df['occupation'] = df['occupation'].replace('SQ002', 'Employee')\n",
    "df['occupation'] = df['occupation'].replace('SQ003', 'Retired')\n",
    "df['occupation'] = df['occupation'].replace('SQ004', 'Unemployed')\n",
    "df['occupation'] = df['occupation'].replace('SQ005', 'Undergrad student')\n",
    "df['occupation'] = df['occupation'].replace('SQ006', 'Graduate student')\n",
    "df['occupation'] = df['occupation'].replace('SQ007', 'Freelancer')\n",
    "df['occupation'] = df['occupation'].replace('SQ008', 'Business owner')\n",
    "df['occupation'] = df['occupation'].replace('SQ009', 'Researcher')\n",
    "df['occupation'] = df['occupation'].replace('SQ010', 'Working Student')\n",
    "\n",
    "df['age'] = df['age'].replace('54 anos ', '54')\n",
    "\n",
    "\n",
    "# Define a dictionary for the replacements\n",
    "replacements = {\n",
    "    'AO01': '1',\n",
    "    'AO02': '2',\n",
    "    'AO03': '3'  # or more\n",
    "}\n",
    "\n",
    "df[['children_0-5', 'children_6-10', 'children_11']] = (\n",
    "    df[['children_0-5', 'children_6-10', 'children_11']]\n",
    "    .replace(replacements)\n",
    "    .fillna(0)  # Replace NaN with 0\n",
    "    .astype(int)  # Convert to integer type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Final Clean_: Some Adjustments May Be Required After New Raw Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Residency and Nationality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming variable nationality\n",
    "df['nationality'] = df2['What is your country of origin?'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning variables, that were open-ended \n",
    "\n",
    "# removing the leading or trailing spaces of the open-ended answers of residence city and country\n",
    "df['residence_city'] = df['residence_city'].str.strip()\n",
    "df['residence_country'] = df['residence_country'].str.strip()\n",
    "\n",
    "#normalization of all text inputs! (residence_city and residence_country), removing accents and putting everything in lowercase\n",
    "from unidecode import unidecode\n",
    "\n",
    "df['residence_city'] = df['residence_city'].apply(lambda x: unidecode(x.lower().strip()))\n",
    "df['residence_country'] = df['residence_country'].apply(lambda x: unidecode(x.lower().strip()))\n",
    "df['nationality'] = df['nationality'].apply(lambda x: unidecode(x.lower().strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in column 'residence_city': ['lisbon' 'cascais' 'luanda' 'sintra' 'covilha' 'sao paulo' 'coimbra'\n",
      " 'almada' 'oeiras' 'natal/rn' 'natal' 'landegem' 'barcelona' 'amersfoort'\n",
      " 'porto' 'corroios' 'basel' 'manaus' 'rio de janeiro' 'guarda' 'stockholm'\n",
      " 'belo horizonte' 'brussels' 'madrid' 'budapest' 'lisboa' 'ghent'\n",
      " 'helsinki' 'aveiro' 'gothenburg' 'athens' 'munich' 'dorchester' 'arganil'\n",
      " 'seixal' 'braga' 'city' 'manaus-am' 'copenhagen' 'halifax' 'sao carlos'\n",
      " 'parede' 'enschede' 'santiago' 'munster' 'mafra' 'bremen' 'mainz'\n",
      " 'wiesbaden' 'freiburg' 'hanover' 'essen' 'lorrach' 'frankfurt am main'\n",
      " 'milano' 'haifa' 'kuwait' 'cologne' 'hamburg' '2' 'villars-sur-glane'\n",
      " 'setubal' 'stuttgart' 'tokyo' 'dublin' 'atlanta' 'london' 'recife'\n",
      " 'utrecht' 'tartu' 'madalena' 'antwerp' 'cascais - lisbon' 'florianopolis'\n",
      " 'berlin' 'fortaleza/ceara']\n",
      "Unique values in column 'residence_country': ['portugal' 'angola' 'brasil' 'brazil' 'belgium' 'spain' 'the netherlands'\n",
      " 'switzerland' 'sweden' 'hungary' 'finland' 'greece' 'germany'\n",
      " 'united kingdom' 'country' 'denmark' 'canada' 'netherlands' 'chile'\n",
      " 'potugal' 'italy' 'israel' 'kuwait' '2' 'schweiz' 'japan' 'ireland'\n",
      " 'united states' 'estonia' 'alemanha']\n"
     ]
    }
   ],
   "source": [
    "# check if there are any redundancies on the cities or countries (open-ended), particularly to check if there's anything written in a different way or a different language\n",
    "unique_cities = df['residence_city'].unique()\n",
    "unique_countries = df['residence_country'].unique()\n",
    "\n",
    "print(f\"Unique values in column 'residence_city': {unique_cities}\")\n",
    "print(f\"Unique values in column 'residence_country': {unique_countries}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alteração de todas as cidades e países \n",
    "# Create a mapping dictionary for normalization\n",
    "city_mapping = {'lisboa': 'lisbon', 'natal/rn':'natal','manaus-am': 'manaus','frankfurt am main': 'frankfurt','cascais - lisbon': 'cascais','city': 'other','2': 'other'}\n",
    "country_mapping = {'brasil':'brazil','the netherlands':'netherlands','potugal':'portugal','country': 'other','2':'other','alemanha':'germany', 'schweiz':'switzerland'}\n",
    "\n",
    "# Apply the mapping to normalize the city and country names\n",
    "df['residence_city'] = df['residence_city'].replace(city_mapping)\n",
    "df['residence_country'] = df['residence_country'].replace(country_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Other Variables and Its Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in column 'children': ['N' 'Y']\n",
      "Unique values in column 'children_0-5': [0 1 2]\n",
      "Unique values in column 'children_6-10': [0 1 2]\n",
      "Unique values in column 'children_11': [0 2 1 3]\n",
      "Unique values in column 'pet_Y_dog': [nan 'Y']\n",
      "Unique values in column 'pet_Y_other': [nan 'Y']\n",
      "Unique values in column 'pet_N': ['Y' nan]\n",
      "Unique values in column 'disability_Y_mobility': [nan 'Y']\n",
      "Unique values in column 'disability_Y_visual': [nan]\n",
      "Unique values in column 'disability_Y_hearing': [nan]\n",
      "Unique values in column 'disability_Y_other': [nan 'Y']\n",
      "Unique values in column 'disability_N': ['Y' nan]\n",
      "Unique values in column 'occupation': ['Graduate student' 'Employee' 'Unemployed' 'Working Student'\n",
      " 'Business owner' 'Researcher' 'Freelancer' 'Retired' 'Undergrad student']\n",
      "Unique values in column 'specialist': ['N' 'Y']\n",
      "Unique values in column 'role_academic': [nan 'Y']\n",
      "Unique values in column 'role_professional': [nan 'Y']\n",
      "Unique values in column 'role_public': [nan 'Y']\n",
      "Unique values in column 'role_private': [nan 'Y']\n",
      "Unique values in column 'role_nonprofit': [nan 'Y']\n",
      "Unique values in column 'role_technology': [nan 'Y']\n",
      "Unique values in column 'role_other': [nan 'bank' 'architect and urban planner' 'Volunteer mapper']\n",
      "Unique values in column 'commute_Walking': ['Y' nan]\n",
      "Unique values in column 'commute_Train': [nan 'Y']\n",
      "Unique values in column 'commute_Subway': [nan 'Y']\n",
      "Unique values in column 'commute_Bus': [nan 'Y']\n",
      "Unique values in column 'commute_Private Car': [nan 'Y']\n",
      "Unique values in column 'commute_Motorcycle': [nan 'Y']\n",
      "Unique values in column 'commute_Bicycle': ['Y' nan]\n",
      "Unique values in column 'commute_Scooter': [nan 'Y']\n",
      "Unique values in column 'commute_TVDE': [nan 'Y']\n",
      "Unique values in column 'commute_home': [nan 'Y']\n",
      "Unique values in column 'commute_Other': [nan 'Bus' 'tram' 'Boat' 'Tramway' 's(peed)-Pedelec'\n",
      " 'I work from home most of the time, but when I commute to the office I use the train, subway, TVDE and I walk on foot. Occasionally I use the bicycle or the bus.'\n",
      " 'Carsharing']\n",
      "Unique values in column 'commute_time': ['<15' '15-30' nan '+45' '30-45' 'home']\n",
      "Unique values in column 'own_car': ['N' 'Y']\n",
      "Unique values in column 'parking_home': [nan 'permit' 'free' 'not free']\n",
      "Unique values in column 'parking_work': [nan 'permit' 'not free' 'free']\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each column and print the unique values\n",
    "\n",
    "for column in df.columns[-36:]:\n",
    "    unique_values = df[column].unique()\n",
    "    print(f\"Unique values in column '{column}': {unique_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan]\n",
      "[nan]\n"
     ]
    }
   ],
   "source": [
    "# Find unique values in the 'commute_other' column\n",
    "unique_disability_visual = df['disability_Y_visual'].unique()\n",
    "unique_disability_hearing = df['disability_Y_hearing'].unique()\n",
    "\n",
    "# Print unique values\n",
    "print(unique_disability_visual)\n",
    "print(unique_disability_hearing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Until the moment (188 answers), there are no answers to some of the columns, so i deleted those\n",
    "# columns that dont have information and also remove the \"more\" since they have phrases with commas, and no relevant input was done \n",
    "df = df.drop(['disability_Y_visual','disability_Y_hearing'], axis=1)\n",
    "\n",
    "df = df.drop(df.filter(regex='_more$').columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan 'Bus' 'tram' 'Boat' 'Tramway' 's(peed)-Pedelec'\n",
      " 'I work from home most of the time, but when I commute to the office I use the train, subway, TVDE and I walk on foot. Occasionally I use the bicycle or the bus.'\n",
      " 'Carsharing']\n"
     ]
    }
   ],
   "source": [
    "# Find unique values in the 'commute_other' column\n",
    "unique_commute_others = df['commute_Other'].unique()\n",
    "\n",
    "# Print unique values\n",
    "print(unique_commute_others)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Clean File to Further Encode It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:\\\\Users\\\\maryi\\\\Desktop\\\\Tese Data Analysis\\\\data_clean.csv'\n",
    "\n",
    "# Export the DataFrame to a .csv file\n",
    "df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
